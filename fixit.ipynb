{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vincent4u/Machine-learning-files/blob/main/fixit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_JX1pYKHxpY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# uploading the csv_file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# naming the csv_file\n",
        "game_csv =\"/content/ce889_dataCollection.csv\"\n",
        "data1 = pd.read_csv(game_csv)\n",
        "data1"
      ],
      "metadata": {
        "id": "oRZvZU8HqNov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Initialize the MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Apply normalization to the DataFrame\n",
        "data1_normalized = pd.DataFrame(scaler.fit_transform(data1), columns=data1.columns)\n",
        "\n",
        "#shuffle the dataframe\n",
        "data1_normalized1 = data1_normalized.sample(frac = 1)\n",
        "\n",
        "# Print the normalized DataFrame\n",
        "print(data1_normalized)\n",
        "print(data1_normalized1)\n"
      ],
      "metadata": {
        "id": "jhM2Cwo7OaaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=data1_normalized1.iloc[:,0]\n",
        "y=data1_normalized1.iloc[:,1]\n",
        "vx=data1_normalized1.iloc[:,2]\n",
        "vy=data1_normalized1.iloc[:,3]\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=1)\n",
        "vx_train, vx_test, vy_train, vy_test = train_test_split(vx, vy, test_size=0.20, random_state=1)"
      ],
      "metadata": {
        "id": "g8yTsk3NwSAk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.array(x_train).reshape(-1, 1)\n",
        "print(x_train)\n",
        "y_train = np.array(y_train).reshape(-1, 1)\n",
        "vx_train = np.array(vx_train).reshape(-1, 1)\n",
        "vy_train = np.array(vy_train).reshape(-1, 1)"
      ],
      "metadata": {
        "id": "c6IMNcBS4X5C",
        "outputId": "803137a2-167d-4ae9-80d5-e28e9ada17de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.42480716]\n",
            " [0.4626579 ]\n",
            " [0.49407573]\n",
            " ...\n",
            " [0.50438578]\n",
            " [0.55621112]\n",
            " [0.52191239]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class NN:\n",
        "    def __init__(self, learn_rate=0.5, input_nodes=len(x_train), hidden_nodes=len(x_train), output_nodes=len(x_train)):\n",
        "        self.input_nodes = input_nodes\n",
        "        self.hidden_nodes = hidden_nodes\n",
        "        self.output_nodes = output_nodes\n",
        "        self.learn_rate = learn_rate\n",
        "\n",
        "        # Defining the layers and attributing random weights for each layer's nodes\n",
        "        layers = [self.input_nodes] + [self.hidden_nodes] + [self.output_nodes]\n",
        "        self.weight = []\n",
        "        for i in range(len(layers) - 1):\n",
        "            w = np.random.rand(layers[i], layers[i+1])\n",
        "            self.weight.append(w)\n",
        "\n",
        "    def __sigmoid(self, hidden_input):\n",
        "        return 1 / (1 + np.exp(-1 * self.learn_rate * hidden_input))\n",
        "\n",
        "    def forward_propagation(self, input):\n",
        "        self.activations = [input]\n",
        "        X_input = input\n",
        "        for i, w in enumerate(self.weight):\n",
        "            hidden_input = np.dot(X_input, w)\n",
        "            activated_hidden_input = self.__sigmoid(hidden_input)\n",
        "            self.activations.append(activated_hidden_input)\n",
        "            X_input = activated_hidden_input\n",
        "            #print(\"Weight matrix at layer {}: \\n{}\".format(i+1, w))\n",
        "        return activated_hidden_input\n",
        "\n",
        "    def backward_propagation(self, targets):\n",
        "        deltas = [None] * len(self.weight)\n",
        "        error = targets - self.activations[-1]\n",
        "        delta = error * self.activations[-1] * (1 - self.activations[-1])\n",
        "        deltas[-1] = delta\n",
        "        for i in reversed(range(len(deltas)-1)):\n",
        "            error = np.dot(deltas[i+1], self.weight[i+1].T)\n",
        "            delta = error * self.activations[i+1] * (1 - self.activations[i+1])\n",
        "            deltas[i] = delta\n",
        "        for i in range(len(self.weight)):\n",
        "            gradient = np.dot(self.activations[i].T, deltas[i])\n",
        "            self.weight[i] += self.learn_rate * gradient\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    mlp = NN()\n",
        "    #mlp.train(x_train, y_train, vx_train, vy_train)\n",
        "    #input = x_train,y_train\n",
        "    output = mlp.forward_propagation(x_train)\n",
        "    #targets = np.random.rand(mlp.output_nodes)\n",
        "\n",
        "\n",
        "\n",
        "    #print(\"The inputs are: {}\".format(input))\n",
        "    #print(\"The outputs are: {}\".format(output))\n",
        "   # print(\"The targets are: {}\".format(targets))\n",
        "\n",
        "    #mlp.backward_propagation(targets)\n",
        "    #print(\"Updated weights after backward propagation:\")\n",
        "    #for i, w in enumerate(mlp.weight):\n",
        "        #print(\"Weight matrix at layer {}: \\n{}\".format(i+1, w))"
      ],
      "metadata": {
        "id": "SfIC3uE53aFo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}